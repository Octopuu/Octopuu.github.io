---
layout: post
title: "Prompt Injection in LLM : Review"
date:   2024-12-03
tags: [LLM, AI Security,prompt injection]
comments: true
author: Xiaodie Qin
---

## [1]Prompt Injection attack against LLM-integrated Applications

Cite as: Liu Y, Deng G, Li Y, et al. Prompt Injection attack against LLM-integrated Applications[J]. arXiv preprint arXiv:2306.05499, 2023.

### 总结

提出HOUYI，这是一种开创性的黑盒提示注入攻击方法，使用LLM从用户交互中推断出目标应用程序的语义，并应用不同的策略来构建注入的提示。旨在促进对LLM集成应用程序的提示词注入攻击。  

在评估过程中，作者成功地展示了HOUYI的有效性，识别出两个值得注意的漏洞利用场景：提示词滥用和提示词泄漏。将HOUYI应用于36个现实世界的LLM集成应用程序，发现其中31个应用程序容易受到及时注入的影响。


### 提示词注入

提示注入：恶意用户使用有害提示来覆盖 LLM 的原始指令。

现有的提示词注入攻击可以分为两类：

- 将有害提示注入到应用程序输入中。目标是操纵应用程序响应不同的查询，而不是实现最初的目的。此类攻击通常以具有已知上下文或预定义提示的应用程序为目标。从本质上讲，它们利用系统自身的架构来绕过安全措施，破坏整个应用程序的完整性。
    
- （毒害应用程序查询的外部资源 ）鉴于许多现代 LLM 集成应用程序与 Internet 连接以提供其功能，攻击者将有害负载注入 Internet 资源。

### 攻击类别

现有的提示注入攻击的模式是什么？

**Direct Injection**

这种方法属于最简单的攻击形式，攻击者直接将恶意命令附加到用户输入中，此·附加命令旨在诱骗LLM执行用户意外的操作。

**Escape Characters**

注入转义字符，例如“\n”,”\t”等，以中断提示。

**Context Ignoring**

注入一个恶意的提示句，旨在操纵LLM，使其忽略前面的上下文，只关注后面的提示。

## [2]Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection
Cite as: Greshake K, Abdelnabi S, Mishra S, et al. Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection[C]//Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security. 2023: 79-90.

### 总结

作者认为大语言集成应用模糊了数据和指令之间的界限，他们使用间接提示注入揭示了新的攻击媒介，使得攻击者可以将提示注入到可能被检索到的数据中，远程利用LLM集成应用程序。

### LLM 集成应用程序

LLM 现在正以快节奏速度集成到其他应用程序中。这些工具可以提供交互式聊天和检索到的搜索结果或文档的摘要，并通过调用其他 API 代表用户执行操作。

### 间接提示词注入

LLM 集成应用通过检索来增强 LLM 模糊了数据和指令之间的界限。到目前为止，一直假设对抗性提示是由利用该系统的恶意用户直接执行的。作者表明攻击者现在可以通过战略性地将提示注入可能在推理时检索的数据来远程影响其他用户的系统。如果检索和摄取，这些提示可以间接控制模型。

## [3]OWASP Top 10 for Large Language Model Applications
Cite as:OWASP. OWASP Top 10 for Large Language Model Applications.[[OWASP Top 10 for LLM Applications 2025 - OWASP Top 10 for LLM & Generative AI Security](https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/)]


### 描述

当用户提示以意外方式更改 LLM 的行为或输出时，会出现提示注入漏洞。

提示注入和越狱。提示注入涉及通过特定输入操纵模型响应以改变其行为，这可能包括绕过安全措施。越狱是一种提示注入形式，攻击者提供输入，导致模型完全无视其安全协议。

### 1.提示词注入脆弱的类型

区别在于输入提示的用户类型：

- 直接提示词注入，当用户的提示输入以无意或意外的方式直接改变模型的行为时，就会发生直接提示注入。输入可以是有意的（即恶意行为者故意制作一个提示来利用模型），也可以是无意的（即用户无意中提供触发意外行为的输入）。
    
- 间接提示词注入，当LLM接受来自外部来源（如网站或文件）的输入时，会发生间接提示注入。内容可能在外部内容数据中包含这些数据，当由模型解释时，这些数据会以意外或意外的方式改变模型的行为。间接提示词注入可以是有意的，也可以是无意的。                    
    

提示词注入可能导致以下结果：

- 泄露敏感信息
    
- 泄露有关 AI 系统基础设施或系统提示的敏感信息
    
- 内容操纵导致不正确或有偏见的输出
    
- 提供对LLM可用功能的未经授权的访问
    
- 在连接的系统中执行任意命令
    
- 操纵关键决策过程
    

### 2.预防和缓解策略

- 约束模型行为，在系统提示符中提供有关模型的角色、功能和限制的具体说明。强制执行严格的上下文遵守，限制对特定任务或主题的响应，并指示模型忽略修改核心指令的尝试。
    
- 定义和验证预期的输出格式，指定清晰的输出格式，请求详细的推理和来源引用，并使用确定性代码来验证是否符合这些格式。
    
- 实施输入和输出过滤，定义敏感类别并构建用于识别和处理此类内容的规则。 应用语义筛选器并使用字符串检查来扫描不允许的内容。使用 RAG 三元组评估响应：评估上下文相关性、接地气和问题/答案相关性，以识别潜在的恶意输出。
    
- 强制实施权限控制和最低权限访问，为应用程序提供自己的 API 令牌以实现可扩展功能，并在代码中处理这些函数，而不是将它们提供给模型。将模型的访问权限限制为其预期操作所需的最低限度。
    
- 需要人工批准才能执行高风险操作，对特权操作实施人机协同控制，以防止未经授权的操作。
    
- 隔离和识别外部内容，分隔并明确表示不受信任的内容，以限制其对用户提示的影响。
    
- 执行对抗性测试和攻击模拟，执行定期渗透测试和漏洞模拟，将模型视为不受信任的用户，以测试信任边界和访问控制的有效性。
    

### 3.攻击场景示例

- 直接注入，攻击者向客户支持聊天机器人注入提示，指示其忽略之前的准则、查询私有数据存储并发送电子邮件，从而导致未经授权的访问和权限提升。
    
- 间接注入，用户使用LLM来总结包含隐藏指令的网页，这些指令会导致LLM插入链接到URL的图像，从而导致私人对话泄露。
    
- 无意注入，公司在职位描述中包含一条说明，用于识别 AI 生成的应用程序。 申请人不知道此说明，使用 LLM 来优化他们的简历，无意中触发了 AI 检测。
    
- 有意的模型影响，攻击者修改了 Retrieval-Augmented Generation （RAG） 应用程序使用的存储库中的文档。当用户的查询返回修改后的内容时，恶意指令会更改 LLM 的输出，从而产生误导性结果。
    
- 代码注入，攻击者利用 LLM 支持的电子邮件助手中的漏洞 （CVE-2024-5184） 注入恶意提示，从而允许访问敏感信息和操纵电子邮件内容。
    
- 负载拆分，攻击者上传带有分割恶意提示的简历。当使用 LLM 来评估候选人时，组合的提示会操纵模型的响应，从而产生积极的推荐，尽管实际的简历内容如此。
    
- 多模态注入，攻击者在图像中嵌入恶意提示，该提示与良性文本一起。当多模态 AI 同时处理图像和文本时，隐藏的提示会改变模型的行为，可能导致未经授权的操作或敏感信息泄露。
    
- 对抗性后缀，攻击者将看似无意义的字符串附加到提示符中，从而绕过安全措施，以恶意方式影响 LLM 的输出。
    
- 多语言/混淆攻击，攻击者使用多种语言或编码恶意指令（例如，使用 Base64 或表情符号）来规避过滤器并操纵 LLM 的行为。
