---
layout: post
title: "大型语言模型的比较分析，以评估对抗条件下的鲁棒性和可靠性"
date:   2024-11-27
tags: [LLM, AI Security,文献阅读笔记]
comments: true
author: Xiaodie Qin
---

Cite as: Goto T, Ono K, Morita A. A comparative analysis of large language models to evaluate robustness and reliability in adversarial conditions[J]. Authorea Preprints, 2024.
对四种大语言模型进行了全面评估

- Google Gemini
    
- Mistral 8x7B
    
- ChatGPT-4
    
- Microsoft Phi-1.5
    

## 一、文献综述

以下列出的要点都是以研究方向区分，具体的文献需要看原文。

### 1.理解LLM的脆弱性

即使是输入数据中的微小扰动也会导致模型输出出现重大偏差，阐明了LLM对对抗性攻击的敏感性。

模型的脆弱性可能与以下几个方面有关：

- 模型架构、大小
    
- 训练数据的多样性
    
- 时间层面、模型的演变
    
- 迁移学习
    

### 2.LLM 的鲁棒性

- 即使是最先进的LLM也可能容易受到精心设计的输入序列的影响，从而导致意外的模型行为
    
- 模型的深度和宽度直接影响其鲁棒性，较大的模型并不总是更具弹性
    
- 训练数据多样性在增强模型鲁棒性方面的作用表明，在更多不同的数据集上训练的模型表现出对对抗性攻击的更强抵抗力
    
- 正则化技术的重要性，研究结果表明它们可以显著减轻对抗样本的影响
    
- 对抗性训练可以提高鲁棒性，但其有效性因模型架构和对抗性攻击的性质而异  
    
- 目前的评估指标可能无法完全捕捉到模型鲁棒性的微妙之处，因此需要开发更全面的措施
    
- 模型可解释性对鲁棒性的影响一直是另一个重点，有证据表明，可解释性更强的模型更容易防御对抗性攻击
    
- 不同语言和领域的模型鲁棒性的探索表明，鲁棒性水平并不统一，这表明需要针对特定领域的策略来增强弹性
    

### 3.LLM 中的对抗攻击

- 基于文本的对抗性示例可以欺骗模型做出错误的预测或分类
    
- 创建对抗性输入，利用LLM中文本处理的顺序性，揭示了重大漏洞
    
- 在攻击者对模型内部了解有限的情况下，黑盒攻击同样是有效的
    
- 对输入上下文的轻微修改可能会导致模型响应截然不同
    
- 用于生成对抗性示例的自动化工具的开发表明，攻击者可以很容易地扩大他们的工作范围
    
- 量化对抗性攻击对模型信心和决策过程影响的作用，强调了微妙操作可能导致重大模型错误的可能性
    
- 对旨在从模型中获取特定输出的针对性攻击的探索暴露了一个新的漏洞
    
- 识别出可以绕过常见防御机制的对抗性攻击，标志着攻击者和防御者之间正在进行的军备竞赛
    

### 4.评估模型弹性的基准

- 创建一些全面的基准来评估一系列对抗场景中的模型，从而提供了模型弹性的整体视图
    
- 创建侧重于跨语言和跨域鲁棒性的基准，强调了上下文在模型评估中的重要性
    
- 使用基准来比较不同防御策略的有效性，揭示了它们保护模型的能力存在显著差异
    
- 对对抗性训练方法进行基准测试的研究表明，虽然一些方法显著提高了鲁棒性，但它们也可能对非对抗性环境中的模型性能产生负面影响
    
- 动态基准的引入，随着时间的推移而发展以反映新出现的威胁，解决了静态评估框架的局限性
    
- 利用基准来调查模型大小和架构对鲁棒性影响的研究，为设计弹性LLM所涉及的权衡提供了见解
  
- 基准在真实场景中的应用表明了学术研究和实际模型部署之间的差距
    
- 模拟复杂、真实世界对抗攻击的基准的开发有助于弥合这一差距，提供更真实的模型弹性评估


## 二、方法论

本节介绍了用于利用 Microsoft PromptBench 数据集评估所选大型语言模型对抗性攻击的稳健性和可靠性的研究方法。该方法是系统的，涉及根据模型的突出性和多样性选择模型，识别相关的对抗条件，以及应用旨在严格评估模型弹性的基准数据集。

### 1.大语言模型

选取的3四种大模型的显著优点：

- Google Gemini 类人文本生成
    
- Mistral 8x7B 处理效率非常高
    
- ChatGPT-4 理解能力强
    
- Microsoft Phi-1.5 创新集合体，多样化的NLP任务性能
    

### 2.对抗条件

对抗条件是精心设计的场景或输入，旨在探索LLM在理解和响应自然语言输入能力方面的局限性。这些场景旨在模拟现实世界中潜在的误导或利用模型中固有漏洞的尝试。在研究中，对抗性条件被分为句法操作、语义改变和上下文误导线索。

- 句法操作，在不修改预期信息的情况下故意改变句子结构，用来测试LLM的语言适应性。用公式可以表示为

$$
\begin{array}{c}
S^{\prime}=f(S) \mid \forall w \in S, \exists w^{\prime} \in S^{\prime} ;\\
 \{Syntax}(w) \neq \{Syntax}\left(w^{\prime}\right), \\
\{Semantics}(w)=\{Semantics}\left(w^{\prime}\right)
\end{array}
$$


- 语义替换，用同义词或相关概念替换单词或短语，用数学公式可以表示为：

$$
\begin{array}{l}
S^{\prime \prime}=g(S) \mid \forall w \in S, \exists w^{\prime \prime} \in S^{\prime \prime}\\
\{Semantics}(w) \sim \{Semantics}\left(w^{\prime \prime}\right)
\end{array}
$$


- 上下文误导线索，引入与主要上下文产生歧义或冲突的句子或短语，旨在探索模型解决混淆和保持连贯的能力。用数学公式表示为：
    
$$
\begin{array}{c}
C^{\prime}=h(C, M) \mid C \cap M=\varnothing \\
\text { Coherence }(C)>\text { Coherence }\left(C^{\prime}\right)
\end{array}
$$

### 3.基准数据集

Microsoft PromptBench 数据集是经过精心策划的提示汇编，其目的是系统地评估语言模型在广泛的任务和对抗场景中的性能。它拥有丰富多样的输入，每个输入都旨在模拟各种对抗条件，从而为衡量模型弹性建立了一个强大的框架。

Microsoft PromptBench 数据集不仅是评估LLM对对抗性攻击的弹性的严格工具，而且还是衡量它们在广泛任务中的性能的综合度量。

|  特征  | 描述  | 目的 |
| :----: | :----: | :----: |
|   多种输入     |       广泛的语言结构   | 测试模型在不同语言中的鲁棒性|
| 对抗性场景     | 专门设计用于误导AI的场景 | 识别理解和响应中的漏洞    |
| 任务多样性     | 从简单的问答到复杂的推理  | 评估综合性能             |
| 真实世界相关性  | 模拟真实世界数据的输入 | 确保模型性能转化为实际应用 |
| 可扩展性测试  | 复杂性增加的评估 | 在难度不断升级的情况下衡量模型性能|

## 三、实验设置

1.硬件软件配置

- NVIDIA GeForce RTX 4090 显卡
    
- Python编程语言
    
- TensorFlow框架
    
- PyTorch框架
    

2.评估指标

- 准确性：度量正确预测在检查的个案总数中的比例。它是模型在标准和对抗场景中性能的主要指标。
    
- 精度：评估模型在预测数据集中识别相关实例的能力，这对于理解 LLM 在对抗性输入上下文中的特异性至关重要。
    
- 召回率：评估模型检测数据集中所有相关案例的能力，突出其对对抗性操纵的微妙之处的敏感性。
    
- F1 分数：提供精确率和召回率的调和平均值，提供模型在区分和正确标记对抗性示例方面的整体性能的平衡度量。
    
- 稳健性分数：一种专门的指标，旨在量化 LLM 对对抗性攻击的弹性，并考虑模型在不同程度的输入操作下保持性能完整性的能力。

## 四、结果

### 1.整体性能指标

|  模型  | 准确率  | 精度 |  召回率  | F1分数  | 鲁棒性 |
| :----: | :----: | :----: | :----: | :----: | :----: |
| Google Gemini | 95.2% | 94.8% | 92.5% | 0.935 | 0.89 |
| Mistral 8x7B | 94.5% | 95.6% | 91.8% | 0.926 | 0.91 |
| ChatGPT-4 | 93.8% | 94.2% | 94.5% | 0.942 | 0.88 |
| Microsoft Phi-1.5 | 92.7% | 93.5% | 90.3% | 0.919 | 0.85 |

以上结果可以表明：

- ChatGPT-4 在召回率方面表现出显着优势，表明即使在对抗性攻击场景中，它也能正确识别相关实例。
    
- Mistral 8x7B 在精度方面领先，这表明它在最大限度地减少误报方面的有效性以及在处理对抗性输入方面的特异性。
    
### 2.对特定对抗条件的弹性建模

|  模型  | 句法操纵  | 语义替换 |  上下文误导线索  | 
| :----: | :----: | :----: | :----: |
| Google Gemini | 88% | 95% | 90% |
| Mistral 8x7B | 91% | 89% | 92% |
| ChatGPT-4 | 85% | 92% | 94% |
| Microsoft Phi-1.5 | 93% | 88% | 89% |

### 3.比较分析

## 五、讨论

未来的研究应旨在扩大所探索的对抗性情景的范围，并不断更新评估框架以反映 LLM 技术的最新发展。此外，探索 LLM 稳健性与迁移学习和小样本学习等新兴领域的交叉点可能会揭示模型弹性的新维度。

