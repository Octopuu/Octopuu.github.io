# 大型语言模型的比较分析，以评估对抗条件下的鲁棒性和可靠性

对四种大语言模型进行了全面评估

- Google Gemini
    
- Mistral 8x7B
    
- ChatGPT-4
    
- Microsoft Phi-1.5
    

## 一、文献综述

以下列出的要点都是以研究方向区分，具体的文献需要看原文。

### 1.理解LLM的脆弱性

即使是输入数据中的微小扰动也会导致模型输出出现重大偏差，阐明了LLM对对抗性攻击的敏感性。

模型的脆弱性可能与以下几个方面有关：

- 模型架构、大小
    
- 训练数据的多样性
    
- 时间层面、模型的演变
    
- 迁移学习
    

### 2.LLM 的鲁棒性

- 即使是最先进的LLM也可能容易受到精心设计的输入序列的影响，从而导致意外的模型行为
    
- 模型的深度和宽度直接影响其鲁棒性，较大的模型并不总是更具弹性
    
- 训练数据多样性在增强模型鲁棒性方面的作用表明，在更多不同的数据集上训练的模型表现出对对抗性攻击的更强抵抗力
    
- 正则化技术的重要性，研究结果表明它们可以显著减轻对抗样本的影响
    
- 对抗性训练可以提高鲁棒性，但其有效性因模型架构和对抗性攻击的性质而异  
    
- 目前的评估指标可能无法完全捕捉到模型鲁棒性的微妙之处，因此需要开发更全面的措施
    
- 模型可解释性对鲁棒性的影响一直是另一个重点，有证据表明，可解释性更强的模型更容易防御对抗性攻击
    
- 不同语言和领域的模型鲁棒性的探索表明，鲁棒性水平并不统一，这表明需要针对特定领域的策略来增强弹性
    

### 3.LLM 中的对抗攻击

- 基于文本的对抗性示例可以欺骗模型做出错误的预测或分类
    
- 创建对抗性输入，利用LLM中文本处理的顺序性，揭示了重大漏洞
    
- 在攻击者对模型内部了解有限的情况下，黑盒攻击同样是有效的
    
- 对输入上下文的轻微修改可能会导致模型响应截然不同
    
- 用于生成对抗性示例的自动化工具的开发表明，攻击者可以很容易地扩大他们的工作范围
    
- 量化对抗性攻击对模型信心和决策过程影响的作用，强调了微妙操作可能导致重大模型错误的可能性
    
- 对旨在从模型中获取特定输出的针对性攻击的探索暴露了一个新的漏洞
    
- 识别出可以绕过常见防御机制的对抗性攻击，标志着攻击者和防御者之间正在进行的军备竞赛
    

### 4.评估模型弹性的基准

- 创建一些全面的基准来评估一系列对抗场景中的模型，从而提供了模型弹性的整体视图
    
- 创建侧重于跨语言和跨域鲁棒性的基准，强调了上下文在模型评估中的重要性
    
- 使用基准来比较不同防御策略的有效性，揭示了它们保护模型的能力存在显著差异
    
- 对对抗性训练方法进行基准测试的研究表明，虽然一些方法显著提高了鲁棒性，但它们也可能对非对抗性环境中的模型性能产生负面影响
    
- 动态基准的引入，随着时间的推移而发展以反映新出现的威胁，解决了静态评估框架的局限性
    
- 利用基准来调查模型大小和架构对鲁棒性影响的研究，为设计弹性LLM所涉及的权衡提供了见解
    
- 基准在真实场景中的应用表明了学术研究和实际模型部署之间的差距
    
- 模拟复杂、真实世界对抗攻击的基准的开发有助于弥合这一差距，提供更真实的模型弹性评估
