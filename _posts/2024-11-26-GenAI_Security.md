---
layout: post
title: "生成式AI安全：挑战和对策"
date:   2024-11-26
tags: [GenAI, AI Security,文献阅读笔记]
comments: true
author: Xiaodie Qin
---


本文深入探究了生成式人工智能所带来的独特安全挑战，并概述了管理这些风险的潜在研究方向。  
Cite as: Zhu B, Mu N, Jiao J, et al. Generative AI security: challenges and countermeasures[J]. arXiv preprint arXiv:2402.12617, 2024.

## 一、与传统安全不同的问题

生成式AI使用户能够快速生成高质量的内容。包括LLM、VLM 和扩散模型。GenAI 模型旨在以超越传统机器学习系统的自主性来理解和生成内容，提供生成文本和代码、与人类和互联网服务交互、生成逼真图像以及理解视觉场景的新功能。

### 1.GenAI 模型容易受到攻击

Jailbreaking 和prompt injection是GenAI模型以及用它们构建的应用程序的两个主要威胁。

- 越狱攻击，使用特制的提示操纵AI生成有害输出
    
- 提示词注入，将恶意数据或指令插入模型的输入流中，诱骗模型遵循攻击者的指示，而不是应用程序开发人员的指示。提示词注入可以利用模型的生成功能来生成明显偏离应用程序预期功能的输出。
    

模型集成到应用程序时，这些漏洞可能会导致严重的安全漏洞。

### 2.对GenAI的错误依赖可能会导致漏洞

GenAI 系统中的漏洞可能不仅仅源于蓄意的对抗行为。在实际场景中，如果GenAI模型生成不安全代码或泄露敏感数据，那么AI在生成或处理敏感数据时，可能会导致新的漏洞。

- 数据泄露风险
    
- 生成不安全代码
    

### 3.GenAI 模型可能被威胁行为者使用

不良行为者可能会使用GenAI创建恶意代码或有害内容，从而对数字安全系统构成重大威胁。

- 制作复杂的网络钓鱼电子邮件，包括自动化创建单独针对的鱼叉式网络钓鱼消息的过程
    
- 为错误信息活动或诈骗生成虚假图像或视频剪辑
    
- 生成能够攻击在线系统的恶意代码
    
- 生成利用GenAI系统“越狱”或绕过其自己的安全协议的提示
    

## 二、现有方法的不足

### 1.GenAI 与ML

DeepMind 最近的一篇报告区分了"广义"和"狭义" AI，并认为LLMs是"广义" AI的第一种形式。GenAI 系统也表现出多种与安全相关的差异，包括：

- 紧急威胁向量：意外的 GenAI 功能可能会产生不可预见的威胁向量。
    
- 扩大攻击面：依赖用户生成的大量数据集进行训练和推理，从而暴露出更大的攻击面。
    
- 深度集成：与其他计算机系统的无中介连接为攻击者提供了更大的目标。
    
- 经济价值：有价值的 GenAI 驱动的应用程序为攻击者构成了更有利可图的目标。
    

### 2.GenAI 与安全性

- Access control
    
- Rule-based blocking
    
- …
    

## 三、潜在研究方向

### 1.AI防火墙

作者建议研究人员研究如何构建“AI 防火墙”，通过监控并可能转换其输入和输出来保护黑盒GenAI模型。例如，用户向 ChatGPT 发送指令，ChatGPT 为 DALL-E 3制作生成图像的提示。ChatGPT 充当AI防火墙。

### 2.集成防火墙

访问GenAI模型的权重，制定防御措施，更有效的检测攻击。

- 内部状态监控，语言模型中的某些神经元或神经元集群可能与幻觉或不道德输出的产生相关。通过监测这些特定的神经元，有可能在响应生成过程的早期检测和减轻不良的模型行为。
    
- 安全微调，开源GenAI模型可以通过监督微调（SFT）或来自人类反馈的强化学习 （RLHF） 针对已知的恶意提示和行为进行微调。这种方法类似于让一个人具备自卫技能，增强模型识别和抵消有害输入的固有能力。在已知威胁的数据集上训练模型将使其能够学习和调整其响应以最大限度地降低风险。
    

### 3.Guardrails

是否有可能对LLM的输出强制实施“guardrails”，即特定于应用程序的限制或策略？

### 4.水印和内容检测

- 对开源模型进行水印处理
    
- 为人工生成的内容添加水印
    
- 跨模型协调
    

### 5.法规执行

### 6.不断发展的威胁管理
